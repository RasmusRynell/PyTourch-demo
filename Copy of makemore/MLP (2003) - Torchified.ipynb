{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f154637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zyrie', 'zyron', 'zzyzx']\n",
      "32033\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "print(words[-3:])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "04a02682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = sorted(list(set(''.join(words)))) # Get all individual letters sorted, should be alphabet (if all letters are once included in our names)\n",
    "num_of_unique_letters = len(letters) + 1 # Add 1 for '.' our special char\n",
    "stoi = {s:i+1 for i,s in enumerate(letters)} # Create a mapping from a char to a int in order to index in tensor\n",
    "stoi['.'] = 0 # Add . as a special char\n",
    "itos = {i:s for s,i in stoi.items()} # Create the mapping in reverse\n",
    "print(num_of_unique_letters)\n",
    "print(letters)\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "991ca757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zzyzx']\n",
      "([[0, 0, 0], [0, 0, 26], [0, 26, 26], [26, 26, 25], [26, 25, 26], [25, 26, 24]], [26, 26, 25, 26, 24, 0])\n"
     ]
    }
   ],
   "source": [
    "def build_data(words, nChars):\n",
    "    x, y = [], []\n",
    "    for w in words:\n",
    "        chs = list('.'*nChars + w + \".\")\n",
    "        chs = [stoi[c] for c in chs]\n",
    "        for i in range(len(chs)-nChars):\n",
    "            x.append(chs[i:i+nChars])\n",
    "            y.append(chs[i+nChars])\n",
    "    return (x,y)\n",
    "\n",
    "# Example 'zuzanna' becomes training example (.. -> z, ..z -> u, and so on until last nna -> .) and chars are converted to the ints\n",
    "\n",
    "nChars = 3\n",
    "print(words[-1:])\n",
    "print(build_data(words[-1:], nChars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "79b158db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "80648b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173724\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "random.shuffle(words)\n",
    "trainIdx = int(0.8*len(words))\n",
    "valIdx = int(0.9*len(words))\n",
    "\n",
    "xTrain, yTrain = build_data(words[0:trainIdx], nChars)\n",
    "xTrain = torch.tensor(xTrain)\n",
    "yTrain = torch.tensor(yTrain)\n",
    "\n",
    "xVal, yVal = build_data(words[trainIdx:valIdx], nChars)\n",
    "xVal = torch.tensor(xVal)\n",
    "yVal = torch.tensor(yVal)\n",
    "\n",
    "xTest, yTest = build_data(words[valIdx:], nChars)\n",
    "xTest = torch.tensor(xTest)\n",
    "yTest = torch.tensor(yTest)\n",
    "\n",
    "\n",
    "nC = 10 # Look up table, can also be seen as the first layer to our network\n",
    "nHiddenN = 200 # Size of hidden layer\n",
    "\n",
    "C = torch.randn((num_of_unique_letters, nC), generator=g)\n",
    "\n",
    "layers = [\n",
    "  torch.nn.Linear(nC*nChars, nHiddenN, bias=False), torch.nn.BatchNorm1d(nHiddenN), torch.nn.Tanh(),\n",
    "  torch.nn.Linear(nHiddenN, nHiddenN, bias=False), torch.nn.BatchNorm1d(nHiddenN), torch.nn.Tanh(),\n",
    "  torch.nn.Linear(nHiddenN, nHiddenN, bias=False), torch.nn.BatchNorm1d(nHiddenN), torch.nn.Tanh(),\n",
    "  torch.nn.Linear(nHiddenN, nHiddenN, bias=False), torch.nn.BatchNorm1d(nHiddenN), torch.nn.Tanh(),\n",
    "  torch.nn.Linear(nHiddenN, nHiddenN, bias=False), torch.nn.BatchNorm1d(nHiddenN), torch.nn.Tanh(),\n",
    "  torch.nn.Linear(nHiddenN, num_of_unique_letters, bias=False), torch.nn.BatchNorm1d(num_of_unique_letters)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "      layer.weight *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "trL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c2af0c56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'retain_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [141], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretain_grad\u001b[49m() \u001b[38;5;66;03m# AFTER_DEBUG: would take out retain_graph\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m     23\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'retain_grad'"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "iters = 200_000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(iters):\n",
    "\n",
    "    # minibatch construct\n",
    "    mIdx = torch.randint(0, xTrain.shape[0], (batch_size,))\n",
    "    Yb = yTrain[mIdx] # batch X,Y\n",
    "    # forward pass\n",
    "\n",
    "    emb = C[xTrain[mIdx]] # embed the characters into vectors\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    \n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 150_000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10_000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{iters:7d}: {loss.item():.4f}')\n",
    "    trL.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1646e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical first loss should be around:\n",
    "-torch.tensor([1/27]).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
